{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.sas.sas7bdat import _column\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_rows',10000)\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "pd.set_option('display.width',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data. Removing the first two columns because they are blank and contains index and timestamp, which are not required for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(\"SUDS.xlsx\")\n",
    "\n",
    "col = df.columns[0]\n",
    "\n",
    "df.drop(columns=col, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'X1..What.do.you.like.best.about..OpenStreetsPGH....check.all.that.apply.':'Best_about_BikePGH'})\n",
    "df = df.rename(columns = {'X2..Have.you.attended.OpenStreetsPGH.before...check.one.':'Attended_Before'})\n",
    "df = df.rename(columns = {'X3..How.did.you.hear.about.OpenStreetsPGH...check.all.that.apply.':'How_you_heard'})\n",
    "df = df.rename(columns = {'X4..What.are.your.primary.activities.at.OpenStreetsPGH..check.up.to.2.':'Primary_activities'})\n",
    "df = df.rename(columns = {'X5..How.long.will.you.spend.at.OSP.today.':'Time_you_will_spend'})\n",
    "df = df.rename(columns = {'X6..Do.you.typically.walk.for.exercise..not.including.OpenStreetsPGH..':'Walking_tendency'})\n",
    "df = df.rename(columns = {'X7..Do.you.typically.bike.for.exercise..not.including.OpenStreetsPGH..':'Biking_tendency'})\n",
    "df = df.rename(columns = {'X8..What.s.your.likelihood.of.walking.for.exercise.in.the.future.':'Likelihood_walking'})\n",
    "df = df.rename(columns = {'X9..What.s.your.likelihood.of.biking.for.exercise.in.the.future.':'Likelihood_biking'})\n",
    "df = df.rename(columns = {'X10..Do.you.typically.walk.for.transportation.trips..e.g..commute..errands..shop..visiting.friends..etc..':'Do_you_walk'})\n",
    "df = df.rename(columns = {'X11..Do.you.typically.Bike.for.transportation.trips.':'Do_you_bike'})\n",
    "df = df.rename(columns = {'X12..What.s.your.likelihood.of.walking.for.transportation...trips.in.the.future.':'Likelihood_walking'})\n",
    "df = df.rename(columns = {'X13..What.s.your.likelihood.of.biking.for.transportation...trips.in.the.future.':'Likelihood_biking'})\n",
    "df = df.rename(columns = {'X14..What.would.get.you.riding.a.bike.more.often.':'Reasons_for_biking'})\n",
    "df = df.rename(columns = {'X15..Yes.or.no..did.you.know.that.the.nonprofit.organization..Bike.Pittsburgh..organizes.OpenstreetsPGH..':'How_you_heard'})\n",
    "df = df.rename(columns = {'Did.you.bring.any.children.to.OSP..if.so.write.the.number.of.childres.next.to.the.corresponding.age.range....0.3.year.old.':'Children_0-3'})\n",
    "df = df.rename(columns = {'Did.you.bring.any.children.to.OSP..if.so.write.the.number.of.childres.next.to.the.corresponding.age.range....4.8.year.old..':'Children_4-8'})\n",
    "df = df.rename(columns = {'Did.you.bring.any.children.to.OSP..if.so.write.the.number.of.childres.next.to.the.corresponding.age.range....9.13.year.old.':'Children_9-13'})\n",
    "df = df.rename(columns = {'Did.you.bring.any.children.to.OSP..if.so.write.the.number.of.childres.next.to.the.corresponding.age.range....14.17.year.old.':'Children_14-17'})\n",
    "df = df.rename(columns = {'X17..Did.you.your.family.spend.any.money.or.do.you.plan.on.spending.any.money.today.at.OpenStreetsPGH.':'Money_spent_in_OSP'})\n",
    "df = df.rename(columns = {'X18.Would.you.attend.another.OpenStreetsPGH.in.the.future.':'Attend_in_future'})\n",
    "df = df.rename(columns = {'Race.or.Ethnicity':'Race'})\n",
    "df = df.rename(columns = {'Gender.Identity':'Gender'})\n",
    "df = df.rename(columns = {'Zip.Code':'ZipCode'})\n",
    "df = df.rename(columns = {'Household.Income':'Income'})\n",
    "df = df.rename(columns = {'city':'City'})\n",
    "df = df.rename(columns = {'state':'State'})\n",
    "df = df.rename(columns = {'latitude':'Latitude'})\n",
    "df = df.rename(columns = {'longitude':'Longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Best_about_BikePGH', 'Attended_Before', 'How_you_heard',\n",
      "       'Primary_activities', 'Time_you_will_spend', 'Walking_tendency',\n",
      "       'Biking_tendency', 'Likelihood_walking', 'Likelihood_biking',\n",
      "       'Do_you_walk', 'Do_you_bike', 'Likelihood_walking', 'Likelihood_biking',\n",
      "       'Reasons_for_biking', 'How_you_heard', 'Children_0-3', 'Children_4-8',\n",
      "       'Children_9-13', 'Children_14-17', 'Money_spent_in_OSP',\n",
      "       'Attend_in_future', 'Age', 'Race', 'Gender', 'ZipCode', 'Income',\n",
      "       'City', 'State', 'Latitude', 'Longitude'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Printing the column names to get the clear picture\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for multiple regression. Mapping the variables to the correct values.\n",
    "In case of variables which are in between some numerical values, I have taken up a calculated estimate.\n",
    "Printing the unique names to get the clear picture of unique values in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'Between $10 and $25' 'Between $25 and $50' 'NONE' 'Less than $10'\n",
      " 'More than $50']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['Attended_Before'] = df['Attended_Before'].map({'This is my first time':0,\n",
    "                                                   'once before':1,\n",
    "                                                   '2-3 times':2,\n",
    "                                                   '4-5 times':4})\n",
    "\n",
    "print(df['Money_spent_in_OSP'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 20. 40. 10.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['Money_spent_in_OSP'] = df['Money_spent_in_OSP'].map({'Between $10 and $25':20,\n",
    "                                                         'Between $25 and $50':40,\n",
    "                                                         'Less than $10':10,\n",
    "                                                         'More than $50:':60})\n",
    "\n",
    "print(df['Money_spent_in_OSP'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'Caucasian' 'African American' 'Irish Italian' 'Hispanic' 'Asian'\n",
      " 'Latina' 'Black']\n"
     ]
    }
   ],
   "source": [
    "df['Race'] = df['Race'].map({'Caucasian ':'Caucasian',\n",
    "                             'Irish Italan': 'Irish Italian',\n",
    "                             'African-American ': 'African American',\n",
    "                             'African American ':'African American',\n",
    "                             'African-American': 'African American',\n",
    "                             'Caucasian / non hispanic':'Caucasian',\n",
    "                             'Asian ':'Asian',\n",
    "                             'European mixed':'Hispanic',\n",
    "                             'Aftican-Indian-American':'African American',\n",
    "                             'Hispanic':'Hispanic',\n",
    "                             'Latina':'Latina',\n",
    "                             'Asian':'Asian',\n",
    "                             'Black':'Black',\n",
    "                             'African American/ caribbean': 'African American',\n",
    "                             'White':'Hispanic',\n",
    "                             })\n",
    "\n",
    "print(df['Race'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12. 15. 16. 17. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32.\n",
      " 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 45. 46. 47. 48. 49. 50. 52.\n",
      " 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 69. 72. 75.\n",
      " 86. 87. nan]\n"
     ]
    }
   ],
   "source": [
    "df['Age'] = df['Age'].replace(199,19)\n",
    "\n",
    "print(df['Age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male' 'Female' nan 'Other']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['Gender'] = df['Gender'].replace('Non-binary/ other','Other')\n",
    "\n",
    "print(df['Gender'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    nan  40000.  20000.  85000.  65000. 135000. 115000. 170000.]\n",
      "[nan 'PA' 'OH' 'NY']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['Income'] = df['Income'].map({'25,000 - 49,999': 40000,\n",
    "                                 '<25,000':20000,\n",
    "                                 '75,000 - 99,999': 85000,\n",
    "                                 '50,000 - 74,999': 65000,\n",
    "                                 '125,000 - 149,000': 135000,\n",
    "                                 '100,000 - 124,999': 115000,\n",
    "                                 '>150,000': 170000})\n",
    "\n",
    "print(df['Income'].unique())\n",
    "\n",
    "print(df['State'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the relevant columns for multiple regression, and dropping all the NaN values so that we get a clear dataframe of objects where each and every data is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Attended_Before Money_spent_in_OSP              Race Age  Gender  Income\n",
      "2                 1                 20         Caucasian  15  Female   40000\n",
      "7                 2                 10     Irish Italian  17    Male   85000\n",
      "14                0                 20             Asian  22  Female   20000\n",
      "15                0                 20             Asian  22    Male   20000\n",
      "24                0                 20         Caucasian  23  Female   20000\n",
      "28                0                 40          Hispanic  25    Male  135000\n",
      "31                4                 10         Caucasian  26    Male   65000\n",
      "33                0                 40  African American  26    Male  135000\n",
      "35                2                 10         Caucasian  27  Female  115000\n",
      "36                0                 10          Hispanic  27  Female   65000\n",
      "37                0                 20            Latina  27  Female   40000\n",
      "38                0                 20  African American  27  Female   40000\n",
      "43                0                 20             Asian  28  Female   85000\n",
      "44                0                 20          Hispanic  28  Female   85000\n",
      "45                4                 20         Caucasian  28  Female   40000\n",
      "47                0                 20             Asian  28  Female   40000\n",
      "48                1                 20         Caucasian  29    Male   85000\n",
      "51                1                 10         Caucasian  29    Male   40000\n",
      "52                4                 10  African American  30    Male   40000\n",
      "57                2                 20         Caucasian  31  Female  170000\n",
      "63                1                 40         Caucasian  32    Male  135000\n",
      "65                2                 20          Hispanic  32    Male  135000\n",
      "67                2                 10         Caucasian  33    Male  115000\n",
      "68                4                 10         Caucasian  33  Female  135000\n",
      "72                1                 10         Caucasian  33  Female  115000\n",
      "77                4                 10         Caucasian  35  Female  170000\n",
      "79                4                 20         Caucasian  35  Female  135000\n",
      "81                0                 20             Asian  36    Male   20000\n",
      "82                2                 40         Caucasian  36    Male  115000\n",
      "88                0                 10         Caucasian  37    Male  170000\n",
      "91                4                 20         Caucasian  38  Female  115000\n",
      "96                0                 10          Hispanic  40  Female   85000\n",
      "97                0                 10         Caucasian  40  Female   85000\n",
      "100               0                 20         Caucasian  42  Female  135000\n",
      "102               1                 20         Caucasian  42  Female   65000\n",
      "106               4                 20         Caucasian  43  Female   40000\n",
      "110               4                 20         Caucasian  47  Female   65000\n",
      "111               0                 40         Caucasian  47  Female  170000\n",
      "113               4                 10         Caucasian  47    Male  170000\n",
      "116               1                 40             Asian  48    Male  135000\n",
      "117               1                 20         Caucasian  48  Female   85000\n",
      "119               2                 10         Caucasian  49  Female   85000\n",
      "121               1                 20         Caucasian  49  Female  170000\n",
      "124               2                 20         Caucasian  50  Female   40000\n",
      "133               1                 40         Caucasian  54  Female  135000\n",
      "134               1                 40         Caucasian  54  Female  135000\n",
      "136               1                 10          Hispanic  55    Male   20000\n",
      "137               4                 10          Hispanic  55  Female   20000\n",
      "138               4                 10          Hispanic  55  Female   20000\n",
      "141               1                 40         Caucasian  56  Female   85000\n",
      "142               0                 20         Caucasian  56    Male   85000\n",
      "154               2                 40         Caucasian  59  Female   20000\n",
      "157               0                 10         Caucasian  60    Male  115000\n",
      "161               0                 10         Caucasian  62   Other   85000\n",
      "164               2                 10  African American  63  Female   65000\n",
      "165               2                 10  African American  63    Male  115000\n",
      "175               2                 40         Caucasian  72  Female   20000\n",
      "178               0                 20         Caucasian  75    Male   65000\n"
     ]
    }
   ],
   "source": [
    "regression_dataframe = pd.DataFrame([df['Attended_Before'], df['Money_spent_in_OSP'],\n",
    "                                     df['Race'], df['Age'], df['Gender'],df['Income']])\n",
    "\n",
    "regression_dataframe = regression_dataframe.transpose()\n",
    "\n",
    "regression_dataframe = regression_dataframe.dropna()\n",
    "\n",
    "print(regression_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dummy variables(Race and Gender).\n",
    "Creating the first basetable(DataFrame to be used for the regression) by dropping the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Money_spent_in_OSP Age  Income  African American  Asian  Caucasian  \\\n",
      "2                   20  15   40000                 0      0          1   \n",
      "7                   10  17   85000                 0      0          0   \n",
      "14                  20  22   20000                 0      1          0   \n",
      "15                  20  22   20000                 0      1          0   \n",
      "24                  20  23   20000                 0      0          1   \n",
      "28                  40  25  135000                 0      0          0   \n",
      "31                  10  26   65000                 0      0          1   \n",
      "33                  40  26  135000                 1      0          0   \n",
      "35                  10  27  115000                 0      0          1   \n",
      "36                  10  27   65000                 0      0          0   \n",
      "37                  20  27   40000                 0      0          0   \n",
      "38                  20  27   40000                 1      0          0   \n",
      "43                  20  28   85000                 0      1          0   \n",
      "44                  20  28   85000                 0      0          0   \n",
      "45                  20  28   40000                 0      0          1   \n",
      "47                  20  28   40000                 0      1          0   \n",
      "48                  20  29   85000                 0      0          1   \n",
      "51                  10  29   40000                 0      0          1   \n",
      "52                  10  30   40000                 1      0          0   \n",
      "57                  20  31  170000                 0      0          1   \n",
      "63                  40  32  135000                 0      0          1   \n",
      "65                  20  32  135000                 0      0          0   \n",
      "67                  10  33  115000                 0      0          1   \n",
      "68                  10  33  135000                 0      0          1   \n",
      "72                  10  33  115000                 0      0          1   \n",
      "77                  10  35  170000                 0      0          1   \n",
      "79                  20  35  135000                 0      0          1   \n",
      "81                  20  36   20000                 0      1          0   \n",
      "82                  40  36  115000                 0      0          1   \n",
      "88                  10  37  170000                 0      0          1   \n",
      "91                  20  38  115000                 0      0          1   \n",
      "96                  10  40   85000                 0      0          0   \n",
      "97                  10  40   85000                 0      0          1   \n",
      "100                 20  42  135000                 0      0          1   \n",
      "102                 20  42   65000                 0      0          1   \n",
      "106                 20  43   40000                 0      0          1   \n",
      "110                 20  47   65000                 0      0          1   \n",
      "111                 40  47  170000                 0      0          1   \n",
      "113                 10  47  170000                 0      0          1   \n",
      "116                 40  48  135000                 0      1          0   \n",
      "117                 20  48   85000                 0      0          1   \n",
      "119                 10  49   85000                 0      0          1   \n",
      "121                 20  49  170000                 0      0          1   \n",
      "124                 20  50   40000                 0      0          1   \n",
      "133                 40  54  135000                 0      0          1   \n",
      "134                 40  54  135000                 0      0          1   \n",
      "136                 10  55   20000                 0      0          0   \n",
      "137                 10  55   20000                 0      0          0   \n",
      "138                 10  55   20000                 0      0          0   \n",
      "141                 40  56   85000                 0      0          1   \n",
      "142                 20  56   85000                 0      0          1   \n",
      "154                 40  59   20000                 0      0          1   \n",
      "157                 10  60  115000                 0      0          1   \n",
      "161                 10  62   85000                 0      0          1   \n",
      "164                 10  63   65000                 1      0          0   \n",
      "165                 10  63  115000                 1      0          0   \n",
      "175                 40  72   20000                 0      0          1   \n",
      "178                 20  75   65000                 0      0          1   \n",
      "\n",
      "     Hispanic  Irish Italian  Latina  Female  Male  Other  \n",
      "2           0              0       0       1     0      0  \n",
      "7           0              1       0       0     1      0  \n",
      "14          0              0       0       1     0      0  \n",
      "15          0              0       0       0     1      0  \n",
      "24          0              0       0       1     0      0  \n",
      "28          1              0       0       0     1      0  \n",
      "31          0              0       0       0     1      0  \n",
      "33          0              0       0       0     1      0  \n",
      "35          0              0       0       1     0      0  \n",
      "36          1              0       0       1     0      0  \n",
      "37          0              0       1       1     0      0  \n",
      "38          0              0       0       1     0      0  \n",
      "43          0              0       0       1     0      0  \n",
      "44          1              0       0       1     0      0  \n",
      "45          0              0       0       1     0      0  \n",
      "47          0              0       0       1     0      0  \n",
      "48          0              0       0       0     1      0  \n",
      "51          0              0       0       0     1      0  \n",
      "52          0              0       0       0     1      0  \n",
      "57          0              0       0       1     0      0  \n",
      "63          0              0       0       0     1      0  \n",
      "65          1              0       0       0     1      0  \n",
      "67          0              0       0       0     1      0  \n",
      "68          0              0       0       1     0      0  \n",
      "72          0              0       0       1     0      0  \n",
      "77          0              0       0       1     0      0  \n",
      "79          0              0       0       1     0      0  \n",
      "81          0              0       0       0     1      0  \n",
      "82          0              0       0       0     1      0  \n",
      "88          0              0       0       0     1      0  \n",
      "91          0              0       0       1     0      0  \n",
      "96          1              0       0       1     0      0  \n",
      "97          0              0       0       1     0      0  \n",
      "100         0              0       0       1     0      0  \n",
      "102         0              0       0       1     0      0  \n",
      "106         0              0       0       1     0      0  \n",
      "110         0              0       0       1     0      0  \n",
      "111         0              0       0       1     0      0  \n",
      "113         0              0       0       0     1      0  \n",
      "116         0              0       0       0     1      0  \n",
      "117         0              0       0       1     0      0  \n",
      "119         0              0       0       1     0      0  \n",
      "121         0              0       0       1     0      0  \n",
      "124         0              0       0       1     0      0  \n",
      "133         0              0       0       1     0      0  \n",
      "134         0              0       0       1     0      0  \n",
      "136         1              0       0       0     1      0  \n",
      "137         1              0       0       1     0      0  \n",
      "138         1              0       0       1     0      0  \n",
      "141         0              0       0       1     0      0  \n",
      "142         0              0       0       0     1      0  \n",
      "154         0              0       0       1     0      0  \n",
      "157         0              0       0       0     1      0  \n",
      "161         0              0       0       0     0      1  \n",
      "164         0              0       0       1     0      0  \n",
      "165         0              0       0       0     1      0  \n",
      "175         0              0       0       1     0      0  \n",
      "178         0              0       0       0     1      0  \n"
     ]
    }
   ],
   "source": [
    "dummy_race = pd.get_dummies(regression_dataframe['Race'])\n",
    "dummy_gender = pd.get_dummies(regression_dataframe['Gender'])\n",
    "\n",
    "basetable = pd.concat([regression_dataframe, dummy_race, dummy_gender], axis = 1, sort = False)\n",
    "basetable = basetable.drop(columns = ['Race', 'Gender','Attended_Before'])\n",
    "print(basetable)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating the independent variable(X) and dependent variables(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considered all the columns except the dependent variable\n",
    "X = basetable.drop(columns = ['Income'])\n",
    "# Our dependent variables\n",
    "y = basetable['Income']\n",
    "# creating the train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=21)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Performing the regression and computing the r-square value and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquare: 0.20835881443278048\n",
      "Root Mean Squared Error: 45760.254435371724\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train,y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"Rsquare: {}\".format(reg_all.score(X_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
